{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d0983d8-5ebc-407a-b65d-fc91d2dc52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/annie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/annie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/annie/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re, unicodedata\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import wikipedia as wk\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9b73b04-7605-40fd-8df8-0b8e2aef9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open('HR.txt','r',errors='ignore')\n",
    "raw=data.read()\n",
    "raw=raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "802969a1-8809-414c-b7b2-db58bd265474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human resource analytics is at the intersection of three bodies of knowledge:\\n\\nhuman resource manage'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "854f4140-e2da-4f13-8283-aa2c24a2f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human resource analytics is at the intersection of three bodies of knowledge:\\n\\nhuman resource management: sets the meaning and purpose of the analytics.',\n",
       " 'data warehousing: knowing how to process and store hr data efficiently, automation of  collection of data and cleaning data.',\n",
       " 'statistical analysis, presentation and interpretation : helps in translating the identified hr  issues into appropriate analyses and communication of results.',\n",
       " '5 fundamental principles of analytics\\n\\nhr analytics is about metrics and measurement.',\n",
       " 'good metrics definitions, both narrative and formulaic, and their documentation are key.',\n",
       " 'a professional and good hr analytics person will have the above bodies of knowledge and know their process and intersection.',\n",
       " 'good communication and collaborative skills are essential.',\n",
       " 'the in-depth expertise in your organization is likely to exist in hrm.',\n",
       " 'it.',\n",
       " 'and decision support.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens=nltk.sent_tokenize(raw)\n",
    "sent_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19021cd5-3049-48b2-95d2-1d0c398b21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    #word tokenization\n",
    "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    \n",
    "    #remove ascii\n",
    "    new_words = []\n",
    "    for word in word_token:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    \n",
    "    #Remove tags\n",
    "    rmv = []\n",
    "    for w in new_words:\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
    "        rmv.append(text)\n",
    "        \n",
    "    #pos tagging and lemmatization\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    rmv = [i for i in rmv if i]\n",
    "    for token, tag in nltk.pos_tag(rmv):\n",
    "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8abf7c70-387f-41f6-80b1-81e39999f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome_input=(\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\",)\n",
    "welcome_response=[\"hi\",\"hey\",\"*nods*\",\"*tips my hat to you*\", \"I'm glad!\"]\n",
    "def welcome(user_response):\n",
    "    for word in user_response.split():\n",
    "        if word.lower in welcome_input:\n",
    "            return random.choice(welcome_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc565d47-053e-46d8-a4eb-cc7797a99727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def generateResponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = linear_kernel(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
    "        print(\"Checking Wikipedia\")\n",
    "        if user_response:\n",
    "            robo_response = wikipedia_data(user_response)\n",
    "            return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "def wikipedia_data(input):\n",
    "    reg_ex = re.search('tell me about (.*)', input)\n",
    "    try:\n",
    "        if reg_ex:\n",
    "            topic = reg_ex.group(1)\n",
    "            wiki = wk.summary(topic, sentences = 3)\n",
    "            return wiki\n",
    "    except Exception as e:\n",
    "            print(\"No content has been found\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40baa434-3f07-4b0d-9809-f16d6c6597bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Chatter and I'm a chatbot\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tell me about Mars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "No content has been found\n",
      "None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is HR?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : hr metrics: application of formulas for measuring and calculating core hr issues so as to draw exact hr results and current scenario of organisation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Tell me about Mars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "No content has been found\n",
      "None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tell me about the planet mars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "No content has been found\n",
      "None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " planet Mars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " SOlar system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "None\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " solar system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatterbot : Checking Wikipedia\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"My name is Chatter and I'm a chatbot\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Chatterbot : You are welcome..\")\n",
    "        else:\n",
    "            if(welcome(user_response)!=None):\n",
    "                print(\"Chatterbot : \"+welcome(user_response))\n",
    "            else:\n",
    "                print(\"Chatterbot : \",end=\"\")\n",
    "                print(generateResponse(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Chatterbot : Bye!!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e08c2-363c-4b44-9ed0-20edc9756173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3442b2-ad6d-4161-8c8b-82db67905f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a978b-7289-406e-bf5a-068bad572752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a19a6-d5e1-4290-88ed-a696b63440c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
